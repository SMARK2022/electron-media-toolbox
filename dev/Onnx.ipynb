{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d959e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading checkpoint from: F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\python\\packages\\LAR_IQA\\checkpoint_epoch_3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_30040\\2710682411.py:50: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1120 22:48:02.254000 30040 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Exporting ONNX to: out\\lar_iqa.onnx\n",
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ProgramData\\miniforge3\\envs\\ML\\lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"d:\\ProgramData\\miniforge3\\envs\\ML\\lib\\site-packages\\onnxscript\\version_converter\\_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"d:\\ProgramData\\miniforge3\\envs\\ML\\lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"d:\\ProgramData\\miniforge3\\envs\\ML\\lib\\site-packages\\onnx\\version_converter.py\", line 37, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: D:\\a\\onnx\\onnx\\onnx\\onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 184 of general pattern rewrite rules.\n",
      "[INFO] ONNX export finished.\n",
      "[INFO] ONNX model saved at: out\\lar_iqa.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# 确保能 import 到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据你的 ipynb 所在路径调整\n",
    "\n",
    "from python.packages.LAR_IQA.scripts.utils import load_model\n",
    "\n",
    "\n",
    "def export_lar_iqa_onnx(\n",
    "    checkpoint_path: str = \"../python/packages/LAR_IQA/checkpoint_epoch_3.pt\",\n",
    "    out_dir: str = \"./out\",\n",
    "    onnx_name: str = \"lar_iqa.onnx\",\n",
    "    use_cuda: bool = True,\n",
    "):\n",
    "    # 1. 选择设备\n",
    "    device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 2. 加载模型\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading checkpoint from: {ckpt_path}\")\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    # 3. 构造 dummy 输入（与 preprocess_image 输出形状一致）\n",
    "    #\n",
    "    # preprocess_image 中：\n",
    "    #   image_authentic: Resize 到 (384, 384)\n",
    "    #   image_synthetic: CenterCrop 到 (1280, 1280)\n",
    "    #\n",
    "    # 所以 dummy 输入分别是 [1, 3, 384, 384] 和 [1, 3, 1280, 1280]\n",
    "    image_authentic = torch.randn(1, 3, 384, 384, device=device)\n",
    "    image_synthetic = torch.randn(1, 3, 1280, 1280, device=device)\n",
    "\n",
    "    # 4. 确保导出目录存在\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_path = out_path / onnx_name\n",
    "\n",
    "    print(f\"[INFO] Exporting ONNX to: {onnx_path}\")\n",
    "\n",
    "    # 5. 导出 ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (image_authentic, image_synthetic),  # 模型的两个输入\n",
    "        onnx_path.as_posix(),\n",
    "        export_params=True,  # 保存权重到 ONNX\n",
    "        opset_version=17,  # 常用的较新 opset 版本（你也可以改成 16/18）\n",
    "        do_constant_folding=True,  # 常量折叠优化\n",
    "        input_names=[\"image_authentic\", \"image_synthetic\"],\n",
    "        output_names=[\"score\"],\n",
    "        dynamic_axes={  # 只把 batch 维做成动态，空间尺寸固定\n",
    "            \"image_authentic\": {0: \"batch_size\"},\n",
    "            \"image_synthetic\": {0: \"batch_size\"},\n",
    "            \"score\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] ONNX export finished.\")\n",
    "    print(f\"[INFO] ONNX model saved at: {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "# 在 ipynb 中直接跑这一段即可导出\n",
    "if __name__ == \"__main__\":\n",
    "    export_lar_iqa_onnx()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
