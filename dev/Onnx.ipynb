{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d959e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading checkpoint from: F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\python\\packages\\LAR_IQA\\checkpoint_epoch_3.pt\n",
      "[INFO] Exporting ONNX to: out\\lar_iqa.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_30040\\1189450358.py:50: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 184 of general pattern rewrite rules.\n",
      "[INFO] ONNX export finished.\n",
      "[INFO] ONNX model saved at: out\\lar_iqa.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# 确保能 import 到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据你的 ipynb 所在路径调整\n",
    "\n",
    "from python.packages.LAR_IQA.scripts.utils import load_model\n",
    "\n",
    "\n",
    "def export_lar_iqa_onnx(\n",
    "    checkpoint_path: str = \"../python/packages/LAR_IQA/checkpoint_epoch_3.pt\",\n",
    "    out_dir: str = \"./out\",\n",
    "    onnx_name: str = \"lar_iqa.onnx\",\n",
    "    use_cuda: bool = True,\n",
    "):\n",
    "    # 1. 选择设备\n",
    "    device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 2. 加载模型\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading checkpoint from: {ckpt_path}\")\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    # 3. 构造 dummy 输入（与 preprocess_image 输出形状一致）\n",
    "    #\n",
    "    # preprocess_image 中：\n",
    "    #   image_authentic: Resize 到 (384, 384)\n",
    "    #   image_synthetic: CenterCrop 到 (1280, 1280)\n",
    "    #\n",
    "    # 所以 dummy 输入分别是 [1, 3, 384, 384] 和 [1, 3, 1280, 1280]\n",
    "    image_authentic = torch.randn(1, 3, 384, 384, device=device)\n",
    "    image_synthetic = torch.randn(1, 3, 1280, 1280, device=device)\n",
    "\n",
    "    # 4. 确保导出目录存在\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_path = out_path / onnx_name\n",
    "\n",
    "    print(f\"[INFO] Exporting ONNX to: {onnx_path}\")\n",
    "\n",
    "    # 5. 导出 ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (image_authentic, image_synthetic),  # 模型的两个输入\n",
    "        onnx_path.as_posix(),\n",
    "        export_params=True,  # 保存权重到 ONNX\n",
    "        opset_version=18,  # 常用的较新 opset 版本（你也可以改成 16/18）\n",
    "        do_constant_folding=True,  # 常量折叠优化\n",
    "        input_names=[\"image_authentic\", \"image_synthetic\"],\n",
    "        output_names=[\"score\"],\n",
    "        dynamic_axes={  # 只把 batch 维做成动态，空间尺寸固定\n",
    "            \"image_authentic\": {0: \"batch_size\"},\n",
    "            \"image_synthetic\": {0: \"batch_size\"},\n",
    "            \"score\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] ONNX export finished.\")\n",
    "    print(f\"[INFO] ONNX model saved at: {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "# 在 ipynb 中直接跑这一段即可导出\n",
    "if __name__ == \"__main__\":\n",
    "    export_lar_iqa_onnx()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da7aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading PyTorch model from F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\python\\packages\\LAR_IQA\\checkpoint_epoch_3.pt onto cuda\n",
      "[RESULT][PyTorch] mean 43.43 ms  std 24.45 ms  (n=200)\n",
      "[INFO] Creating ONNX Runtime session for F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\dev\\out\\lar_iqa.onnx with providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "[RESULT][ONNX] mean 38.61 ms  std 2.50 ms  (n=200)\n",
      "[INFO] max abs diff between PyTorch and ONNX outputs: 0.000155\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Benchmark PyTorch vs ONNX inference (20 runs average)\n",
    "\n",
    "# 配置\n",
    "runs = 200\n",
    "warmups = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# If you want to benchmark the PyTorch model, provide checkpoint path (same as export used)\n",
    "ckpt_path = Path(\"../python/packages/LAR_IQA/checkpoint_epoch_3.pt\").resolve()\n",
    "\n",
    "# Prepare dummy inputs (与 export 中一致)\n",
    "input_torch = (\n",
    "    torch.randn(1, 3, 384, 384, device=device),\n",
    "    torch.randn(1, 3, 1280, 1280, device=device),\n",
    ")\n",
    "\n",
    "def bench_torch(model, inputs, runs=runs, warmups=warmups):\n",
    "    model.eval()\n",
    "    # warm-up\n",
    "    for _ in range(warmups):\n",
    "        with torch.no_grad():\n",
    "            _ = model(*inputs)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = model(*inputs)\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0) * 1000.0)  # ms\n",
    "    return np.mean(times), np.std(times), times\n",
    "\n",
    "def bench_onnx(session, np_inputs, runs=runs, warmups=warmups):\n",
    "    input_names = [i.name for i in session.get_inputs()]\n",
    "    # warm-up\n",
    "    for _ in range(warmups):\n",
    "        _ = session.run(None, {name: arr for name, arr in zip(input_names, np_inputs)})\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = session.run(None, {name: arr for name, arr in zip(input_names, np_inputs)})\n",
    "        t1 = time.perf_counter()\n",
    "        times.append((t1 - t0) * 1000.0)  # ms\n",
    "    return np.mean(times), np.std(times), times\n",
    "\n",
    "# ---------- PyTorch benchmark ----------\n",
    "if ckpt_path.exists():\n",
    "    print(f\"[INFO] Loading PyTorch model from {ckpt_path} onto {device}\")\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    mean_torch, std_torch, times_torch = bench_torch(model, input_torch)\n",
    "    print(f\"[RESULT][PyTorch] mean {mean_torch:.2f} ms  std {std_torch:.2f} ms  (n={runs})\")\n",
    "else:\n",
    "    print(f\"[WARN] Checkpoint not found at {ckpt_path}. Skipping PyTorch benchmark.\")\n",
    "\n",
    "# ---------- ONNX benchmark ----------\n",
    "onnx_path = \"./out/lar_iqa.onnx\"  # path from export step\n",
    "onnx_file = Path(onnx_path).resolve()  # onnx_path provided in notebook\n",
    "if not onnx_file.exists():\n",
    "    raise FileNotFoundError(f\"ONNX model not found: {onnx_file}\")\n",
    "\n",
    "# select providers\n",
    "available_providers = ort.get_available_providers()\n",
    "providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if \"CUDAExecutionProvider\" in available_providers else [\"CPUExecutionProvider\"]\n",
    "sess_opts = ort.SessionOptions()\n",
    "sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "print(f\"[INFO] Creating ONNX Runtime session for {onnx_file} with providers={providers}\")\n",
    "sess = ort.InferenceSession(onnx_file.as_posix(), sess_opts, providers=providers)\n",
    "\n",
    "# prepare numpy inputs (move to CPU and convert to float32)\n",
    "np_inputs = [inp.detach().cpu().numpy().astype(np.float32) if isinstance(inp, torch.Tensor) else np.array(inp, dtype=np.float32) for inp in input_torch]\n",
    "\n",
    "mean_onnx, std_onnx, times_onnx = bench_onnx(sess, np_inputs)\n",
    "print(f\"[RESULT][ONNX] mean {mean_onnx:.2f} ms  std {std_onnx:.2f} ms  (n={runs})\")\n",
    "\n",
    "# optional: compare outputs once to ensure correctness\n",
    "with torch.no_grad():\n",
    "    pt_out = None\n",
    "    if ckpt_path.exists():\n",
    "        pt_out = model(*input_torch)\n",
    "onnx_out = sess.run(None, {name: arr for name, arr in zip([i.name for i in sess.get_inputs()], np_inputs)})\n",
    "if pt_out is not None:\n",
    "    # convert pt_out and onnx_out[0] to numpy for comparison\n",
    "    pt_np = pt_out.detach().cpu().numpy() if isinstance(pt_out, torch.Tensor) else np.array(pt_out)\n",
    "    onnx_np = np.array(onnx_out[0])\n",
    "    diff = np.max(np.abs(pt_np - onnx_np))\n",
    "    print(f\"[INFO] max abs diff between PyTorch and ONNX outputs: {diff:.6f}\")\n",
    "else:\n",
    "    print(\"[INFO] PyTorch output not available to compare with ONNX.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
