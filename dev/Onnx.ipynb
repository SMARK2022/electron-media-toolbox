{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d959e04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[INFO] Loading checkpoint from: F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\bak\\LAR_IQA\\checkpoint_epoch_3.pt\n",
      "[INFO] Exporting ONNX to: out\\lar_iqa.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_37112\\2003838692.py:50: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 184 of general pattern rewrite rules.\n",
      "[INFO] ONNX export finished.\n",
      "[INFO] ONNX model saved at: out\\lar_iqa.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# 确保能 import 到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据你的 ipynb 所在路径调整\n",
    "\n",
    "from bak.LAR_IQA.scripts.utils import load_model\n",
    "\n",
    "\n",
    "def export_lar_iqa_onnx(\n",
    "    checkpoint_path: str = \"../bak/LAR_IQA/checkpoint_epoch_3.pt\",\n",
    "    out_dir: str = \"./out\",\n",
    "    onnx_name: str = \"lar_iqa.onnx\",\n",
    "    use_cuda: bool = True,\n",
    "):\n",
    "    # 1. 选择设备\n",
    "    device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 2. 加载模型\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading checkpoint from: {ckpt_path}\")\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    # 3. 构造 dummy 输入（与 preprocess_image 输出形状一致）\n",
    "    #\n",
    "    # preprocess_image 中：\n",
    "    #   image_authentic: Resize 到 (384, 384)\n",
    "    #   image_synthetic: CenterCrop 到 (1280, 1280)\n",
    "    #\n",
    "    # 所以 dummy 输入分别是 [1, 3, 384, 384] 和 [1, 3, 1280, 1280]\n",
    "    image_authentic = torch.randn(1, 3, 384, 384, device=device)\n",
    "    image_synthetic = torch.randn(1, 3, 1280, 1280, device=device)\n",
    "\n",
    "    # 4. 确保导出目录存在\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_path = out_path / onnx_name\n",
    "\n",
    "    print(f\"[INFO] Exporting ONNX to: {onnx_path}\")\n",
    "\n",
    "    # 5. 导出 ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (image_authentic, image_synthetic),  # 模型的两个输入\n",
    "        onnx_path.as_posix(),\n",
    "        export_params=True,  # 保存权重到 ONNX\n",
    "        opset_version=18,  # 常用的较新 opset 版本（你也可以改成 16/18）\n",
    "        do_constant_folding=True,  # 常量折叠优化\n",
    "        input_names=[\"image_authentic\", \"image_synthetic\"],\n",
    "        output_names=[\"score\"],\n",
    "        dynamic_axes={  # 只把 batch 维做成动态，空间尺寸固定\n",
    "            \"image_authentic\": {0: \"batch_size\"},\n",
    "            \"image_synthetic\": {0: \"batch_size\"},\n",
    "            \"score\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] ONNX export finished.\")\n",
    "    print(f\"[INFO] ONNX model saved at: {onnx_path}\")\n",
    "    return onnx_path\n",
    "\n",
    "\n",
    "# 在 ipynb 中直接跑这一段即可导出\n",
    "if __name__ == \"__main__\":\n",
    "    export_lar_iqa_onnx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44e65e",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b708cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device for export: cuda\n",
      "[INFO] Loading checkpoint from: F:\\ML\\PythonAIProject\\SMARKMediaTools_web\\electron-media-toolbox\\bak\\LAR_IQA\\checkpoint_epoch_3.pt\n",
      "[INFO] Exporting ONNX to: out\\lar_iqa.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Temp\\ipykernel_39988\\2984897581.py:103: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `MobileNetMerged([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 184 of general pattern rewrite rules.\n",
      "[INFO] ONNX export finished.\n",
      "[INFO] Raw ONNX model saved at: out\\lar_iqa.onnx\n",
      "[INFO] Start ORT offline optimization -> out\\lar_iqa_opt.onnx (level=extended)\n",
      "[ORT] Use providers: ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "[INFO] ORT graph optimization finished.\n",
      "[INFO] Optimized ONNX model saved at: out\\lar_iqa_opt.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import torch\n",
    "import onnxruntime as ort  # 新增：用于离线图优化\n",
    "\n",
    "# 确保能 import 到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据你的 ipynb 所在路径调整\n",
    "\n",
    "from bak.LAR_IQA.scripts.utils import load_model\n",
    "\n",
    "\n",
    "def _pick_ort_providers(prefer: Literal[\"directml\", \"cpu\", \"auto\"] = \"auto\"):\n",
    "    \"\"\"\n",
    "    根据当前环境选择合适的 onnxruntime providers：\n",
    "    - 优先 DirectML（如果安装了 onnxruntime-directml 并可用）\n",
    "    - 否则退回 CPUExecutionProvider\n",
    "    \"\"\"\n",
    "    available = ort.get_available_providers()\n",
    "    providers: list[str]\n",
    "\n",
    "    if prefer in (\"directml\", \"auto\") and \"DmlExecutionProvider\" in available:\n",
    "        providers = [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "        print(f\"[ORT] Use providers: {providers}\")\n",
    "    else:\n",
    "        providers = [\"CPUExecutionProvider\"]\n",
    "        print(f\"[ORT] Use providers: {providers}\")\n",
    "\n",
    "    return providers\n",
    "\n",
    "\n",
    "def _pick_optimization_level(\n",
    "    level: Literal[\"disable\", \"basic\", \"extended\", \"all\"] = \"extended\",\n",
    ") -> ort.GraphOptimizationLevel:\n",
    "    \"\"\"\n",
    "    将字符串映射到 onnxruntime.GraphOptimizationLevel。\n",
    "    默认使用 extended：basic + extended 的图融合，兼容 CPU / DirectML。\n",
    "    \"\"\"\n",
    "    if level == \"disable\":\n",
    "        return ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "    if level == \"basic\":\n",
    "        return ort.GraphOptimizationLevel.ORT_ENABLE_BASIC\n",
    "    if level == \"all\":\n",
    "        # all = basic + extended + layout（NCHWc，CPU 专用）\n",
    "        return ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    # 默认 extended\n",
    "    return ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "\n",
    "def export_lar_iqa_onnx(\n",
    "    checkpoint_path: str = \"../bak/LAR_IQA/checkpoint_epoch_3.pt\",\n",
    "    out_dir: str = \"./out\",\n",
    "    onnx_name: str = \"lar_iqa.onnx\",\n",
    "    use_cuda: bool = True,\n",
    "    # 新增：是否做 ORT 离线图优化\n",
    "    optimize_with_ort: bool = True,\n",
    "    # 新增：图优化等级（推荐 \"extended\"）\n",
    "    optimization_level: Literal[\"disable\", \"basic\", \"extended\", \"all\"] = \"extended\",\n",
    "    # 新增：优化时偏好的 provider（\"directml\" / \"cpu\" / \"auto\"）\n",
    "    optimize_provider: Literal[\"directml\", \"cpu\", \"auto\"] = \"auto\",\n",
    "    # 新增：优化后模型文件名（None 则使用 `<stem>_opt.onnx`）\n",
    "    optimized_name: Optional[str] = None,\n",
    "):\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 1. 选择 PyTorch 导出时使用的设备（只影响导出阶段的 dummy forward）\n",
    "    # ---------------------------------------------------------------------------\n",
    "    device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n",
    "    print(f\"[INFO] Using device for export: {device}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 2. 加载 PyTorch 模型\n",
    "    # ---------------------------------------------------------------------------\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    print(f\"[INFO] Loading checkpoint from: {ckpt_path}\")\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 3. 构造 dummy 输入（与 preprocess_image 输出形状一致）\n",
    "    #    image_authentic: [1, 3, 384, 384]\n",
    "    #    image_synthetic: [1, 3, 1280, 1280]\n",
    "    # ---------------------------------------------------------------------------\n",
    "    image_authentic = torch.randn(1, 3, 384, 384, device=device)\n",
    "    image_synthetic = torch.randn(1, 3, 1280, 1280, device=device)\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 4. 确保导出目录存在\n",
    "    # ---------------------------------------------------------------------------\n",
    "    out_path = Path(out_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    onnx_path = out_path / onnx_name\n",
    "\n",
    "    print(f\"[INFO] Exporting ONNX to: {onnx_path}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 5. 使用 torch.onnx.export 导出原始 ONNX\n",
    "    # ---------------------------------------------------------------------------\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (image_authentic, image_synthetic),  # 模型的两个输入\n",
    "        onnx_path.as_posix(),\n",
    "        export_params=True,  # 保存权重到 ONNX\n",
    "        opset_version=18,  # 你之前使用的 opset\n",
    "        do_constant_folding=True,  # 常量折叠优化\n",
    "        input_names=[\"image_authentic\", \"image_synthetic\"],\n",
    "        output_names=[\"score\"],\n",
    "        dynamic_axes={  # 只把 batch 维做成动态，空间尺寸固定\n",
    "            \"image_authentic\": {0: \"batch_size\"},\n",
    "            \"image_synthetic\": {0: \"batch_size\"},\n",
    "            \"score\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] ONNX export finished.\")\n",
    "    print(f\"[INFO] Raw ONNX model saved at: {onnx_path}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # 6. 可选：使用 ONNX Runtime 做一次“离线图优化”，生成 *_opt.onnx\n",
    "    # ---------------------------------------------------------------------------\n",
    "    if not optimize_with_ort:\n",
    "        print(\"[INFO] Skip ORT graph optimization (optimize_with_ort=False).\")\n",
    "        return onnx_path\n",
    "\n",
    "    # 6.1 选择优化后的输出路径\n",
    "    if optimized_name is None:\n",
    "        optimized_onnx_path = onnx_path.with_name(onnx_path.stem + \"_opt.onnx\")\n",
    "    else:\n",
    "        optimized_onnx_path = out_path / optimized_name\n",
    "\n",
    "    # 6.2 配置 SessionOptions\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = _pick_optimization_level(optimization_level)\n",
    "    # 设置离线优化输出路径：初始化 Session 时会把优化后的模型写到这里\n",
    "    sess_options.optimized_model_filepath = optimized_onnx_path.as_posix()\n",
    "\n",
    "    print(f\"[INFO] Start ORT offline optimization -> {optimized_onnx_path} (level={optimization_level})\")\n",
    "\n",
    "    # 6.3 选择 provider（优先 DirectML，否则 CPU）\n",
    "    providers = _pick_ort_providers(optimize_provider)\n",
    "\n",
    "    # 6.4 创建 Session（只为触发优化与序列化，不必真正推理）\n",
    "    _ = ort.InferenceSession(\n",
    "        onnx_path.as_posix(),\n",
    "        sess_options,\n",
    "        providers=providers,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] ORT graph optimization finished.\")\n",
    "    print(f\"[INFO] Optimized ONNX model saved at: {optimized_onnx_path}\")\n",
    "\n",
    "    return optimized_onnx_path\n",
    "\n",
    "\n",
    "# 在 ipynb 中直接跑这一段即可导出\n",
    "if __name__ == \"__main__\":\n",
    "    export_lar_iqa_onnx()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c90df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 找到 57 张测试图片。示例：\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5655-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5656-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5718-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5719-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5720-NEF.jpg\n",
      "\n",
      "[INFO] 预处理所有图片（preprocess_image, RGB）...\n",
      "[INFO] 预处理完成。\n",
      "\n",
      "[PyTorch] device = cuda\n",
      "[PyTorch] Total: 1.8677s, Per image: 32.766 ms\n",
      "[ONNX] Loaded: lar_iqa.onnx, providers = ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "[ONNX-raw] Total: 1.2670s, Per image: 22.229 ms\n",
      "[ONNX] Loaded: lar_iqa_opt.onnx, providers = ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "[ONNX-opt] Total: 1.2622s, Per image: 22.143 ms\n",
      "\n",
      "[ONNX-raw] vs PyTorch 误差统计：\n",
      "  MAE  = 0.000316\n",
      "  RMSE = 0.000377\n",
      "  Max |Δ| = 0.000941\n",
      "\n",
      "[ONNX-opt] vs PyTorch 误差统计：\n",
      "  MAE  = 0.000316\n",
      "  RMSE = 0.000377\n",
      "  Max |Δ| = 0.000941\n",
      "\n",
      "========== 推理速度对比（57 张图） ==========\n",
      "PyTorch   : total 1.8677s, per image 32.766 ms\n",
      "ONNX-raw  : total 1.2670s, per image 22.229 ms\n",
      "ONNX-opt  : total 1.2622s, per image 22.143 ms\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 对比 LAR-IQA 的 PyTorch / ONNX / ONNX-OPT 的数值误差与推理速度\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "# 让 Python 找到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据 notebook 所在位置适当调整\n",
    "\n",
    "# 根据你的工程结构选择其中一个导入方式：\n",
    "# from packages.LAR_IQA.scripts.utils import load_model, preprocess_image\n",
    "from bak.LAR_IQA.scripts.utils import load_model, preprocess_image\n",
    "\n",
    "\n",
    "# ----------------- 路径配置：按需修改 -----------------\n",
    "\n",
    "CHECKPOINT_PATH = \"../bak/LAR_IQA/checkpoint_epoch_3.pt\"\n",
    "ONNX_RAW_PATH = \"./out/lar_iqa.onnx\"\n",
    "ONNX_OPT_PATH = \"./out/lar_iqa_opt.onnx\"\n",
    "\n",
    "# 测试图片目录：请改成你实际存放测试照片的文件夹\n",
    "IMAGE_DIR = r\"N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\"  # TODO: 修改为你自己的图片目录\n",
    "\n",
    "# ONNX provider 配置（按需切换）\n",
    "# - 只用 CPU: [\"CPUExecutionProvider\"]\n",
    "# - CUDA: [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "# - DirectML: [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "# ORT_PROVIDERS = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]  # 如果你想测 DirectML 就改成 [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "ORT_PROVIDERS = [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]  # 如果你想测 DirectML 就改成 [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "\n",
    "\n",
    "# ----------------- 帮助函数 -----------------\n",
    "\n",
    "\n",
    "def collect_image_paths(folder: str) -> List[Path]:\n",
    "    exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"]\n",
    "    folder = Path(folder)\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"IMAGE_DIR 不存在: {folder.resolve()}\")\n",
    "    paths: List[Path] = []\n",
    "    for ext in exts:\n",
    "        paths.extend(folder.rglob(f\"*{ext}\"))\n",
    "    paths = sorted(paths)\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"在 {folder.resolve()} 下没有找到任何图片文件\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def preprocess_all_images(image_paths: List[Path]):\n",
    "    \"\"\"\n",
    "    使用原来的 preprocess_image（PIL+torchvision），\n",
    "    在 CPU 上预处理一次，后面 Torch / ONNX 都重用这些张量。\n",
    "    返回：List[(path, image_auth_cpu, image_syn_cpu)]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for p in image_paths:\n",
    "        # 始终在 CPU 上预处理，确保 .numpy() 可用\n",
    "        img_auth_cpu, img_syn_cpu = preprocess_image(str(p), \"RGB\", \"cpu\")\n",
    "        results.append((p, img_auth_cpu, img_syn_cpu))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_torch_inference(\n",
    "    preprocessed,\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "):\n",
    "    print(f\"\\n[PyTorch] device = {device}\")\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # warmup\n",
    "    with torch.no_grad():\n",
    "        p0, auth0_cpu, syn0_cpu = preprocessed[0]\n",
    "        auth0 = auth0_cpu.to(device)\n",
    "        syn0 = syn0_cpu.to(device)\n",
    "        _ = model(auth0, syn0)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, auth_cpu, syn_cpu in preprocessed:\n",
    "            auth = auth_cpu.to(device, non_blocking=True)\n",
    "            syn = syn_cpu.to(device, non_blocking=True)\n",
    "            out = model(auth, syn)\n",
    "            score = float(out.detach().cpu().item())\n",
    "            scores.append(score)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / len(preprocessed)\n",
    "    print(f\"[PyTorch] Total: {total_time:.4f}s, Per image: {avg_time * 1000:.3f} ms\")\n",
    "\n",
    "    return np.array(scores, dtype=np.float32), total_time, avg_time\n",
    "\n",
    "\n",
    "def make_onnx_session(onnx_path: str, providers):\n",
    "    onnx_path = Path(onnx_path).resolve()\n",
    "    if not onnx_path.exists():\n",
    "        raise FileNotFoundError(f\"ONNX model not found: {onnx_path}\")\n",
    "    sess_options = ort.SessionOptions()\n",
    "    # 这里只做推理，不再做离线图优化，因此不设 optimized_model_filepath\n",
    "    sess = ort.InferenceSession(onnx_path.as_posix(), sess_options, providers=providers)\n",
    "    print(f\"[ONNX] Loaded: {onnx_path.name}, providers = {sess.get_providers()}\")\n",
    "    return sess\n",
    "\n",
    "\n",
    "def run_onnx_inference(\n",
    "    preprocessed,\n",
    "    session: ort.InferenceSession,\n",
    "    label: str = \"ONNX\",\n",
    "):\n",
    "    scores = []\n",
    "\n",
    "    # warmup\n",
    "    p0, auth0_cpu, syn0_cpu = preprocessed[0]\n",
    "    warm_inputs = {\n",
    "        \"image_authentic\": auth0_cpu.numpy(),\n",
    "        \"image_synthetic\": syn0_cpu.numpy(),\n",
    "    }\n",
    "    _ = session.run(None, warm_inputs)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _, auth_cpu, syn_cpu in preprocessed:\n",
    "        inputs = {\n",
    "            \"image_authentic\": auth_cpu.numpy(),\n",
    "            \"image_synthetic\": syn_cpu.numpy(),\n",
    "        }\n",
    "        outputs = session.run(None, inputs)\n",
    "        # outputs[0] 预期形状为 [1, 1] 或 [1]\n",
    "        score = float(np.array(outputs[0]).reshape(-1)[0])\n",
    "        scores.append(score)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / len(preprocessed)\n",
    "    print(f\"[{label}] Total: {total_time:.4f}s, Per image: {avg_time * 1000:.3f} ms\")\n",
    "\n",
    "    return np.array(scores, dtype=np.float32), total_time, avg_time\n",
    "\n",
    "\n",
    "def summarize_error(\n",
    "    ref_scores: np.ndarray,\n",
    "    test_scores: np.ndarray,\n",
    "    name: str,\n",
    "):\n",
    "    diff = test_scores - ref_scores\n",
    "    abs_diff = np.abs(diff)\n",
    "    mae = abs_diff.mean()\n",
    "    max_abs = abs_diff.max()\n",
    "    rmse = np.sqrt((diff**2).mean())\n",
    "    print(f\"\\n[{name}] vs PyTorch 误差统计：\\n  MAE  = {mae:.6f}\\n  RMSE = {rmse:.6f}\\n  Max |Δ| = {max_abs:.6f}\")\n",
    "\n",
    "\n",
    "# ----------------- 主流程 -----------------\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) 收集测试图片\n",
    "    image_paths = collect_image_paths(IMAGE_DIR)\n",
    "    print(f\"[INFO] 找到 {len(image_paths)} 张测试图片。示例：\")\n",
    "    for p in image_paths[:5]:\n",
    "        print(\"   -\", p)\n",
    "\n",
    "    # 2) 统一预处理（CPU）\n",
    "    print(\"\\n[INFO] 预处理所有图片（preprocess_image, RGB）...\")\n",
    "    preprocessed = preprocess_all_images(image_paths)\n",
    "    print(\"[INFO] 预处理完成。\")\n",
    "\n",
    "    # 3) PyTorch 推理\n",
    "    torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch_scores, t_torch, avg_torch = run_torch_inference(\n",
    "        preprocessed,\n",
    "        CHECKPOINT_PATH,\n",
    "        torch_device,\n",
    "    )\n",
    "\n",
    "    # 4) ONNX 原始模型推理\n",
    "    sess_raw = make_onnx_session(ONNX_RAW_PATH, ORT_PROVIDERS)\n",
    "    onnx_raw_scores, t_raw, avg_raw = run_onnx_inference(\n",
    "        preprocessed,\n",
    "        sess_raw,\n",
    "        label=\"ONNX-raw\",\n",
    "    )\n",
    "\n",
    "    # 5) ONNX 优化模型推理\n",
    "    sess_opt = make_onnx_session(ONNX_OPT_PATH, ORT_PROVIDERS)\n",
    "    onnx_opt_scores, t_opt, avg_opt = run_onnx_inference(\n",
    "        preprocessed,\n",
    "        sess_opt,\n",
    "        label=\"ONNX-opt\",\n",
    "    )\n",
    "\n",
    "    # 6) 数值误差统计（相对 PyTorch）\n",
    "    summarize_error(torch_scores, onnx_raw_scores, name=\"ONNX-raw\")\n",
    "    summarize_error(torch_scores, onnx_opt_scores, name=\"ONNX-opt\")\n",
    "\n",
    "    # 7) 速度对比汇总\n",
    "    n = len(preprocessed)\n",
    "    print(\"\\n========== 推理速度对比（{} 张图） ==========\".format(n))\n",
    "    print(f\"PyTorch   : total {t_torch:.4f}s, per image {avg_torch * 1000:.3f} ms\")\n",
    "    print(f\"ONNX-raw  : total {t_raw:.4f}s, per image {avg_raw * 1000:.3f} ms\")\n",
    "    print(f\"ONNX-opt  : total {t_opt:.4f}s, per image {avg_opt * 1000:.3f} ms\")\n",
    "\n",
    "\n",
    "# 在 notebook 中直接调用 main() 即可\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f5697",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870f3b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 找到 57 张测试图片。前 5 张示例：\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5655-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5656-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5718-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5719-NEF.jpg\n",
      "   - N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\\Z30_5720-NEF.jpg\n",
      "[Stage-1 Read] Total: 9.6026s, Per image: 168.467 ms, Num: 57\n",
      "[Stage-2 Preprocess-CPU] Total: 11.7436s, Per image: 206.029 ms, Num: 57\n",
      "[Stage-2 Preprocess-GPU] Total: 11.5621s, Per image: 202.844 ms, Num: 57\n",
      "\n",
      "[PyTorch] device = cuda\n",
      "[PyTorch] Total: 1.9552s, Per image: 34.302 ms\n",
      "[ONNX] Loaded: lar_iqa.onnx, providers = ['DmlExecutionProvider', 'CPUExecutionProvider']\n",
      "[ONNX-raw] Total: 25.0422s, Per image: 439.336 ms\n",
      "\n",
      "[ONNX-raw] vs PyTorch 误差统计：\n",
      "  MAE  = 0.000567\n",
      "  RMSE = 0.000681\n",
      "  Max |Δ| = 0.001306\n",
      "\n",
      "========== 整体耗时对比（57 张图） ==========\n",
      "[Stage-1 Read]         Total 9.6026s,   Per image 168.467 ms\n",
      "[Stage-2 Pre-CPU]      Total 11.7436s, Per image 206.029 ms\n",
      "[Stage-2 Pre-GPU]      Total 11.5621s, Per image 202.844 ms\n",
      "[Stage-3 PyTorch]      Total 1.9552s,  Per image 34.302 ms\n",
      "[Stage-3 ONNX-raw]     Total 25.0422s,    Per image 439.336 ms\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 对比 LAR-IQA 的 PyTorch / ONNX 推理结果与耗时：\n",
    "# - 阶段 1：OpenCV 读图时间\n",
    "# - 阶段 2：预处理时间（CPU 与 GPU 各自一份）\n",
    "# - 阶段 3：模型推理时间（PyTorch / ONNX）\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import onnxruntime as ort\n",
    "\n",
    "# 让 Python 找到你的 LAR_IQA 工程\n",
    "sys.path.append(\"..\")  # 根据 notebook 所在位置适当调整\n",
    "\n",
    "from bak.LAR_IQA.scripts.utils import load_model  # 只用 load_model，不再用原始 preprocess_image\n",
    "\n",
    "# ----------------- 路径配置：按需修改 -----------------\n",
    "\n",
    "CHECKPOINT_PATH = \"../bak/LAR_IQA/checkpoint_epoch_3.pt\"\n",
    "ONNX_RAW_PATH = \"./out/lar_iqa.onnx\"\n",
    "\n",
    "# 测试图片目录：请改成你实际存放测试照片的文件夹\n",
    "IMAGE_DIR = r\"N:\\待整理\\2025.10.19 上海\\个人导出\\DxO\\导出\"\n",
    "\n",
    "# ONNX provider 配置（按需切换）\n",
    "# - 只用 CPU: [\"CPUExecutionProvider\"]\n",
    "# - CUDA: [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "# - DirectML: [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "ORT_PROVIDERS = [\"DmlExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "\n",
    "\n",
    "# ----------------- 工具函数 -----------------\n",
    "\n",
    "\n",
    "def collect_image_paths(folder: str) -> List[Path]:\n",
    "    exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"]\n",
    "    folder = Path(folder)\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"IMAGE_DIR 不存在: {folder.resolve()}\")\n",
    "    paths: List[Path] = []\n",
    "    for ext in exts:\n",
    "        paths.extend(folder.rglob(f\"*{ext}\"))\n",
    "    paths = sorted(paths)\n",
    "    if not paths:\n",
    "        raise RuntimeError(f\"在 {folder.resolve()} 下没有找到任何图片文件\")\n",
    "    return paths\n",
    "\n",
    "\n",
    "def cv_imread_unicode(path_str: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用 OpenCV 读取图片，兼容中文/特殊路径。\n",
    "    \"\"\"\n",
    "    data = np.fromfile(path_str, dtype=np.uint8)\n",
    "    img = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"cv_imdecode 读取失败: {path_str}\")\n",
    "    return img  # BGR, uint8\n",
    "\n",
    "\n",
    "def read_all_images_cv2(image_paths: List[Path]):\n",
    "    \"\"\"\n",
    "    阶段 1：统一读图（OpenCV），返回 [(Path, bgr_np)]，并统计总耗时。\n",
    "    \"\"\"\n",
    "    images: List[Tuple[Path, np.ndarray]] = []\n",
    "    t0 = time.perf_counter()\n",
    "    for p in image_paths:\n",
    "        img = cv_imread_unicode(str(p))\n",
    "        images.append((p, img))\n",
    "    t1 = time.perf_counter()\n",
    "    total = t1 - t0\n",
    "    avg = total / len(images)\n",
    "    print(f\"[Stage-1 Read] Total: {total:.4f}s, Per image: {avg * 1000:.3f} ms, Num: {len(images)}\")\n",
    "    return images, total, avg\n",
    "\n",
    "\n",
    "def preprocess_single_from_bgr(\n",
    "    bgr: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    从 OpenCV BGR 图像，按训练时的逻辑生成：\n",
    "      - image_authentic: Resize 到 (384, 384)\n",
    "      - image_synthetic: CenterCrop 到 (1280, 1280)\n",
    "    并做 ToTensor + Normalize，返回 shape=[1,3,H,W] 的 tensor（在指定 device 上）。\n",
    "    \"\"\"\n",
    "    # BGR -> RGB，转为 float32 [0,1]，NCHW\n",
    "    # 注意 .copy() 避免 from_numpy 引用只读内存\n",
    "    rgb = bgr[..., ::-1].copy()  # HWC, RGB\n",
    "    img = torch.from_numpy(rgb).to(device=device, dtype=torch.float32) / 255.0\n",
    "    img = img.permute(2, 0, 1).unsqueeze(0)  # [1,3,H,W]\n",
    "\n",
    "    # authentic: Resize 到 384x384（双线性插值）\n",
    "    image_auth = F.interpolate(img, size=(384, 384), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    # synthetic: CenterCrop 到 1280x1280（不足时对边缘做对称 padding）\n",
    "    _, _, h, w = img.shape\n",
    "    crop_h = crop_w = 1280\n",
    "\n",
    "    if h < crop_h or w < crop_w:\n",
    "        # 对称 padding 到至少 1280，再居中裁剪\n",
    "        pad_top = max((crop_h - h) // 2, 0)\n",
    "        pad_bottom = max(crop_h - h - pad_top, 0)\n",
    "        pad_left = max((crop_w - w) // 2, 0)\n",
    "        pad_right = max(crop_w - w - pad_left, 0)\n",
    "        img_padded = F.pad(img, (pad_left, pad_right, pad_top, pad_bottom))\n",
    "    else:\n",
    "        img_padded = img\n",
    "\n",
    "    _, _, hp, wp = img_padded.shape\n",
    "    top = (hp - crop_h) // 2\n",
    "    left = (wp - crop_w) // 2\n",
    "    image_syn = img_padded[:, :, top : top + crop_h, left : left + crop_w]\n",
    "\n",
    "    # Normalize（ImageNet 均值/方差）\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
    "\n",
    "    image_auth = (image_auth - mean) / std\n",
    "    image_syn = (image_syn - mean) / std\n",
    "\n",
    "    return image_auth, image_syn  # [1,3,H,W]\n",
    "\n",
    "\n",
    "def preprocess_all_images(\n",
    "    images_bgr: List[Tuple[Path, np.ndarray]],\n",
    "    device: torch.device,\n",
    "    label: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    阶段 2：对读入的 BGR 图像做预处理，返回 [(Path, auth_tensor, syn_tensor)]。\n",
    "    auth/syn 张量已经在指定 device 上。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for p, bgr in images_bgr:\n",
    "        auth, syn = preprocess_single_from_bgr(bgr, device)\n",
    "        results.append((p, auth, syn))\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total = t1 - t0\n",
    "    avg = total / len(results)\n",
    "    print(f\"[Stage-2 Preprocess-{label}] Total: {total:.4f}s, Per image: {avg * 1000:.3f} ms, Num: {len(results)}\")\n",
    "    return results, total, avg\n",
    "\n",
    "\n",
    "def run_torch_inference(\n",
    "    preprocessed,\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    阶段 3-A：PyTorch 推理。\n",
    "    preprocessed: [(Path, auth_tensor, syn_tensor)]，auth/syn 已在 device 上。\n",
    "    \"\"\"\n",
    "    print(f\"\\n[PyTorch] device = {device}\")\n",
    "    ckpt_path = Path(checkpoint_path).resolve()\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    model = load_model(str(ckpt_path), False, device)\n",
    "    model.eval()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # warmup\n",
    "    with torch.no_grad():\n",
    "        _, auth0, syn0 = preprocessed[0]\n",
    "        _ = model(auth0, syn0)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, auth, syn in preprocessed:\n",
    "            out = model(auth, syn)\n",
    "            score = float(out.detach().cpu().item())\n",
    "            scores.append(score)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / len(preprocessed)\n",
    "    print(f\"[PyTorch] Total: {total_time:.4f}s, Per image: {avg_time * 1000:.3f} ms\")\n",
    "\n",
    "    return np.array(scores, dtype=np.float32), total_time, avg_time\n",
    "\n",
    "\n",
    "def make_onnx_session(onnx_path: str, providers):\n",
    "    onnx_path = Path(onnx_path).resolve()\n",
    "    if not onnx_path.exists():\n",
    "        raise FileNotFoundError(f\"ONNX model not found: {onnx_path}\")\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess = ort.InferenceSession(onnx_path.as_posix(), sess_options, providers=providers)\n",
    "    print(f\"[ONNX] Loaded: {onnx_path.name}, providers = {sess.get_providers()}\")\n",
    "    return sess\n",
    "\n",
    "\n",
    "def run_onnx_inference(\n",
    "    preprocessed_cpu,\n",
    "    session: ort.InferenceSession,\n",
    "    label: str = \"ONNX\",\n",
    "):\n",
    "    \"\"\"\n",
    "    阶段 3-B：ONNX 推理，使用 CPU 预处理的张量（会自动拷贝到 DML / CUDA）。\n",
    "    preprocessed_cpu: [(Path, auth_cpu, syn_cpu)]，auth/syn 在 CPU 上。\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    # warmup\n",
    "    _, auth0_cpu, syn0_cpu = preprocessed_cpu[0]\n",
    "    warm_inputs = {\n",
    "        \"image_authentic\": auth0_cpu.numpy(),\n",
    "        \"image_synthetic\": syn0_cpu.numpy(),\n",
    "    }\n",
    "    _ = session.run(None, warm_inputs)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _, auth_cpu, syn_cpu in preprocessed_cpu:\n",
    "        inputs = {\n",
    "            \"image_authentic\": auth_cpu.numpy(),\n",
    "            \"image_synthetic\": syn_cpu.numpy(),\n",
    "        }\n",
    "        outputs = session.run(None, inputs)\n",
    "        score = float(np.array(outputs[0]).reshape(-1)[0])\n",
    "        scores.append(score)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    total_time = t1 - t0\n",
    "    avg_time = total_time / len(preprocessed_cpu)\n",
    "    print(f\"[{label}] Total: {total_time:.4f}s, Per image: {avg_time * 1000:.3f} ms\")\n",
    "\n",
    "    return np.array(scores, dtype=np.float32), total_time, avg_time\n",
    "\n",
    "\n",
    "def summarize_error(\n",
    "    ref_scores: np.ndarray,\n",
    "    test_scores: np.ndarray,\n",
    "    name: str,\n",
    "):\n",
    "    diff = test_scores - ref_scores\n",
    "    abs_diff = np.abs(diff)\n",
    "    mae = abs_diff.mean()\n",
    "    max_abs = abs_diff.max()\n",
    "    rmse = np.sqrt((diff**2).mean())\n",
    "    print(f\"\\n[{name}] vs PyTorch 误差统计：\\n  MAE  = {mae:.6f}\\n  RMSE = {rmse:.6f}\\n  Max |Δ| = {max_abs:.6f}\")\n",
    "\n",
    "\n",
    "# ----------------- 主流程 -----------------\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) 收集测试图片\n",
    "    image_paths = collect_image_paths(IMAGE_DIR)\n",
    "    print(f\"[INFO] 找到 {len(image_paths)} 张测试图片。前 5 张示例：\")\n",
    "    for p in image_paths[:5]:\n",
    "        print(\"   -\", p)\n",
    "\n",
    "    # 2) 阶段 1：统一读图（OpenCV）\n",
    "    images_bgr, t_read, avg_read = read_all_images_cv2(image_paths)\n",
    "\n",
    "    # 3) 阶段 2：预处理（CPU & GPU）\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    pre_cpu, t_pre_cpu, avg_pre_cpu = preprocess_all_images(images_bgr, cpu_device, label=\"CPU\")\n",
    "\n",
    "    pre_gpu = None\n",
    "    t_pre_gpu = 0.0\n",
    "    avg_pre_gpu = 0.0\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    if use_cuda:\n",
    "        gpu_device = torch.device(\"cuda\")\n",
    "        pre_gpu, t_pre_gpu, avg_pre_gpu = preprocess_all_images(images_bgr, gpu_device, label=\"GPU\")\n",
    "    else:\n",
    "        print(\"[Stage-2 Preprocess-GPU] CUDA 不可用，跳过 GPU 预处理测试。\")\n",
    "\n",
    "    # 4) 阶段 3-A：PyTorch 推理（优先使用 GPU 预处理）\n",
    "    if use_cuda and pre_gpu is not None:\n",
    "        torch_device = torch.device(\"cuda\")\n",
    "        pre_for_torch = pre_gpu\n",
    "    else:\n",
    "        torch_device = torch.device(\"cpu\")\n",
    "        pre_for_torch = pre_cpu\n",
    "\n",
    "    torch_scores, t_torch, avg_torch = run_torch_inference(\n",
    "        pre_for_torch,\n",
    "        CHECKPOINT_PATH,\n",
    "        torch_device,\n",
    "    )\n",
    "\n",
    "    # 5) 阶段 3-B：ONNX 推理（使用 CPU 预处理）\n",
    "    sess_raw = make_onnx_session(ONNX_RAW_PATH, ORT_PROVIDERS)\n",
    "    onnx_raw_scores, t_raw, avg_raw = run_onnx_inference(\n",
    "        pre_cpu,\n",
    "        sess_raw,\n",
    "        label=\"ONNX-raw\",\n",
    "    )\n",
    "\n",
    "    # 6) 数值误差统计（ONNX vs PyTorch）\n",
    "    summarize_error(torch_scores, onnx_raw_scores, name=\"ONNX-raw\")\n",
    "\n",
    "    # 7) 速度对比汇总（包含读图 & 预处理）\n",
    "    n = len(pre_cpu)\n",
    "    print(\"\\n========== 整体耗时对比（{} 张图） ==========\".format(n))\n",
    "    print(f\"[Stage-1 Read]         Total {t_read:.4f}s,   Per image {avg_read * 1000:.3f} ms\")\n",
    "    print(f\"[Stage-2 Pre-CPU]      Total {t_pre_cpu:.4f}s, Per image {avg_pre_cpu * 1000:.3f} ms\")\n",
    "    if use_cuda:\n",
    "        print(f\"[Stage-2 Pre-GPU]      Total {t_pre_gpu:.4f}s, Per image {avg_pre_gpu * 1000:.3f} ms\")\n",
    "    print(f\"[Stage-3 PyTorch]      Total {t_torch:.4f}s,  Per image {avg_torch * 1000:.3f} ms\")\n",
    "    print(f\"[Stage-3 ONNX-raw]     Total {t_raw:.4f}s,    Per image {avg_raw * 1000:.3f} ms\")\n",
    "\n",
    "\n",
    "# 在 notebook 中直接调用 main() 即可\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
